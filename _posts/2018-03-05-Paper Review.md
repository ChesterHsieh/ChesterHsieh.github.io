# [Notes on Noise Contrastive Estimation and Negative Sampling](https://arxiv.org/pdf/1410.8251.pdf_)

It's kind of wired to write down note for note. In turn this isn't the typical NLP paper. 

 ## Improvement
 Cost on softmax is huge when training the word2vec model.  That's inherietence problem with multiple class proble. Intuitive requirement for the loss function in multi-class problem is entropy for the probability. 

***Reduce computing cost***
## Method

![Probability for ***context** *c* for ***word*** *w*](https://media.licdn.com/dms/image/C5112AQGTvxJFr-qXLA/article-inline_image-shrink_1000_1488/0?e=1557360000&v=beta&t=wkUHIU5P-LPHks6tGQ8sx_zqX5Xuuwi7EymCIBuuc-w)
How to map this problem to the binary classification problem
=> Naive Bayes classifier
We generate Î¸ to simulate real distribution. 

![](https://media.licdn.com/dms/image/C5112AQHERXeLbNM3Lw/article-inline_image-shrink_400_744/0?e=1557360000&v=beta&t=xXg5F_XPxGxLa4-_aisW4Kiw60D7x4bo_AUye0kLZo8)



# [Deep Reinforcement Learning for Dialogue Generation](https://arxiv.org/pdf/1606.01541.pdf)

## Assumption
## Improvement
Given the framework on 2 dialog agent pratice together. 
## 


# Context Sensitive Synonym Discovery for Web Search
## Assumption
user click the same document => query are the same meaning.
## Improvement
Generate the synonym by co-clicked. 
## How?
Measure the similairt by click activity

<!--stackedit_data:
eyJoaXN0b3J5IjpbMTI4NjA5OTEzOCwtMTI1OTE3NDM3OSwtMT
Q0OTQxNDcxMywyMTk4NjAwMDMsLTY2MDE1OTk0LDY4MDIyMDYw
M119
-->