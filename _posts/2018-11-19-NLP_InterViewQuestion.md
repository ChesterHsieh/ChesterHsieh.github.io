---
layout:     post
title:      NLP engineer interview questions collections
subtitle:  Practice make perfect
date:       2018-11-19
author:    Chester
header-img: img/failure.jpg
catalog: true
tags:
    - Job
   - NLP

---
# Question Collection 
# What's word embedding and how to implement?
>[Vector space models](https://en.wikipedia.org/wiki/Vector_space_model) (VSMs) represent (embed) words in a continuous vector space where semantically similar words are mapped to nearby points ('are embedded nearby each other'). VSMs have a long, rich history in NLP, but all methods depend in some way or another on the [Distributional Hypothesis](https://en.wikipedia.org/wiki/Distributional_semantics#Distributional_Hypothesis), which states that words that appear in the same contexts share semantic meaning. The different approaches that leverage this principle can be divided into two categories: _count-based methods_ (e.g. [Latent Semantic Analysis](https://en.wikipedia.org/wiki/Latent_semantic_analysis)), and _predictive methods_ (e.g. [neural probabilistic language models](http://www.scholarpedia.org/article/Neural_net_language_models)).

## count-based model
- LSA
## predictive model

# General Preprocessing Process?

1.  Metamorphosis by Franz Kafka
2.  Text Cleaning is Task Specific
3.  Manual Tokenization
4.  Tokenization and Cleaning with NLTK
5.  Additional Text Cleaning Considerations
6.  Tips for Cleaning Text for Word Embedding

## What is part of speech (POS) tagging? What is the simplest approach to building a POS tagger that you can imagine?

Is one of the main components of almost any NLP analysis. The task of POS-tagging simply implies labelling words with their appropriate Part-Of-Speech (Noun, Verb, Adjective, Adverb, Pronoun, …).

It following another question. Why we need to get part of speech?
Here's is really common and popular application rely on NLP.
->**Chatbot**

## How would you build a POS tagger from scratch given a corpus of annotated sentences? How would you deal with unknown words?
1. [Tnt Tagger](http://www.coli.uni-saarland.de/~thorsten/tnt/)

2. Consider the following analysis involving  woman  (a noun),  bought  (a verb),  over  (a preposition), and  the  (a determiner). The  text.similar()  method takes a word  w, finds all contexts  w1w  w2, then finds all words  w'  that appear in the same context, i.e.  w1w'w2.
If we have big enough corpus we can find enough past date support our analysis.
---
Here's also a very interesting instruction about building Pos tagger.[# A Good Part-of-Speech Tagger in about 200 Lines of Python](
https://explosion.ai/blog/part-of-speech-pos-tagger-in-python)

## How would you train a model that identifies whether the word “Apple” in a sentence belongs to the fruit or the company?
We can apply HMM model to identify the result. Most easy way to implement this is doing n-gram encoding then train the classification model.

## How would you find all the occurrences of quoted text in a news article?
a hand-crafted extractor may not be such a bad thing, especially if you need to implement it quickly. Another option to use some existing rule-processing engine.

## How would you build a system that auto corrects text that has been generated by a speech recognition system?

Here's the one big problem we having standard solution today. [Language model](https://en.wikipedia.org/wiki/Language_model) is one of solution. There's are different between common language model. We need to generate [character-level language model](https://towardsdatascience.com/language-models-spellchecking-and-autocorrection-dd10f739443c)

## What is latent semantic indexing and where can it be applied?
Wait to answer :P

## What are stop words? Describe an application in which stop words should be removed.
Wait to answer :P
## How would you design a model to predict whether a movie review was positive or negative?
Wait to answer :P

## How to implement flexible text matching
 1. Soundex 
 2. Metaphone
 3. Edit Distance
## What is beam search?

# Classifer problem
For lot of small taks on building chatbot, the classifer problem is more common than the regression problem. (hmmm. ranking result is regression though. ...) .
## cost function
- softmax_cross_entropy_with_logits,
- tf.nn.weighted_cross_entropy_with_logits

## Imblanced training data?
[How to handle Imbalanced Classification Problems in machine learning?](https://www.analyticsvidhya.com/blog/2017/03/imbalanced-classification-problem/)
 1. Over sampling /Down sampling /Mixtur / SMOTE
 2.  Ensamble algorithm=> bagging/boosting
 3.  Weighting change. 

##

# Deeplearning !?
Try to list better structure to formula an MECE list to cover most topic I expect for interview. Pretend to be build up the house, here's some idea

## All the bricks 
### Deep learning very basic type?
The real answer here is infinity. However, I don't believe anyone can actually doing that good for knowing everything. 
- MLP, Boltzman machine, CNN,RNN,GAN, LSTM, autoencoder
- pooling layer, Relu Layer, full connected layer, 
- BatchNormalization
### Initialize 
### Activation machine
Sigmoid, Relu, Step, Tanh, softmax
### Cost function
- tf.nn.weighted_cross_entropy_with_logits
- Noise Constrastive Estimation
- 
### Gradiaent descent
Adam ..SGD,,
### Overfitting
batch normalization, dropout, 
### tensorflow 
constant,variable, placeholder, session, 
### Hyper parameter
They are tons of parameter. Not gonna to list it all. 
- Learning rate
- 
## Building process
Tensorflow is the the only library I've used so far.( I assume the keras as part of tensorflow) Following google's talk , they strongerly suggest peoeple to use Keras and other high level tool first. In this post, I hope to write down higher level idea for interview mainly. 
### Backward propagation

## Tricky Question
### Greadient Vanishing & exploding
### Epoch & batch & iteration

## Real Case problme
### T-series problem
### Caption ball in screen
### I dentitfy 

# Advanced Research 
Sometime we need to excting the interviewer and show them we catch up on the trend. 
## Bert
Google provide another state of art embbed way. 
##

# Knowledge supplement :)
While I collect questions and try answer them, I review some key tool. I hope they'll also helpful for reader. 

#### [HMM & POS](http://nbviewer.jupyter.org/github/iit-cs585/main/blob/master/lec/l07/L07.ipynb)
The main idea of Hidden Markov models

The output at time  ii  depends on the input at time  ii  and the output at previous times  i−1i−1,  i−2i−2, ...:

There's are two example using HMM concept.

-   **Language model**:  p(wi|wi−2,wi−1)p(wi|wi−2,wi−1)
-   **Hidden Markov model**:  p(yi|wi,yi−1,yi−2,…)p(yi|wi,yi−1,yi−2,…)
    -   e.g.,  yiyi  is part-of-speech tag at time  i

## bagging & boosting



1. filling the empty 
2. review the old story
 - SVM
 - Random forest
 - Decsion tree
 - A/B test basic
 - Index f-1 precision recall
3. language model
	 - Transformer-Decoder architecture
5. RNN family
[Best grpahical explaination](https://towardsdatascience.com/animated-rnn-lstm-and-gru-ef124d06cf45)
	 - GRU
	 - LSTM
6. Attention model
# Deployment
## Flask API wrapper

# Reference
- [Question Source-1](https://resources.workable.com/natural-language-processing-engineer-interview-questions)
- [Question Source-2]()
- [NLTK for POS](https://www.nltk.org/book/ch05.html)



# TF-serving
1. Serverable
	- Central part of the tf serving
	- Can be push new module when serive is alive.
2. Loader
	- Need to wrap out the servable function.
3. Source
    Eat the module, that's it.
4. Manager
## JAVA wrapper as client service
## Flask
## 

# Shell Script


# Knowledge supplement :)
While I collect questions and try answer them, I review some key tool. I hope they'll also helpful for reader. 

#### [HMM & POS](http://nbviewer.jupyter.org/github/iit-cs585/main/blob/master/lec/l07/L07.ipynb)
The main idea of Hidden Markov models

The output at time  ii  depends on the input at time  ii  and the output at previous times  i−1i−1,  i−2i−2, ...:

There's are two example using HMM concept.

-   **Language model**:  p(wi|wi−2,wi−1)p(wi|wi−2,wi−1)
-   **Hidden Markov model**:  p(yi|wi,yi−1,yi−2,…)p(yi|wi,yi−1,yi−2,…)
    -   e.g.,  yiyi  is part-of-speech tag at time  i

# Reference
- [Question Source-1](https://resources.workable.com/natural-language-processing-engineer-interview-questions)
- [Question Source-2]()
- [NLTK for POS](https://www.nltk.org/book/ch05.html)
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTE2Nzk3Mjk2OTUsLTMzMTA3ODg4NSwtNj
IyNTg2MzY1LDE4NjA4OTk5MywtODEzODA4ODE0LDE1OTM2ODYw
NjEsMzA3MjAzMzQ1LC0xMTMzNTM1MTYxLC0xMjAyNDQ3Nzg0LC
0xNTIyNTIyNDI3LC02OTEyMTUyOTIsLTEyMDQ2MTE4ODEsMTc3
MjY4ODIzMiwxOTAyMjA3MzEwLC0xNDk4NjA0MjQyLDE5OTAzMj
gxMjAsMTUwOTM4NDQxMCwtMzI0ODczNjYsNDk4MDgzMzM1LC04
OTEwMjQ1MzRdfQ==
-->